{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install joblib\n",
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import simhash\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance testing\n",
    "\n",
    "To really benefit from the speed-up of parallelisation we need the quantity ${}^{\\text{blocks}}C _{\\text{bits}}$ (which is the thing we're parallelising over) to be less than the number of workers. \n",
    "\n",
    "Here we're using the number of cores as \"workers\", but these could be different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashes(n, frac_unique):\n",
    "    \"\"\"Get a number of hashes with a certain fraction that are (almost) unique\"\"\"\n",
    "    num_unique = int(n * frac_unique)\n",
    "    num_duplicates = n - num_unique\n",
    "    unique_hashes = [random.randint(0, 1 << 64) for i in range(num_unique)]\n",
    "    duplicate_hashes = random.choices(unique_hashes, k=num_duplicates)\n",
    "    return unique_hashes + duplicate_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = 6\n",
    "bits = 5\n",
    "nck = math.comb(blocks, bits)\n",
    "print(f\"Number of permutations: {nck}\")\n",
    "\n",
    "num_hashes = [1_000_000, 5_000_000, 10_000_000]\n",
    "frac_unique = [0.8, 0.9, 0.95, 1]\n",
    "\n",
    "iter_list = list(product(num_hashes, frac_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = {\n",
    "    \"time_taken_find_all\": [],\n",
    "    \"time_taken_perm\": [],\n",
    "    \"num_hashes\": [],\n",
    "    \"fraction_unique\": []\n",
    "}\n",
    "skip_checks = True\n",
    "\n",
    "for n, f in iter_list:\n",
    "    hashes = get_hashes(n, f)\n",
    "\n",
    "    # Time find_all:\n",
    "    t0 = time.perf_counter()\n",
    "    matches = simhash.find_all(hashes, blocks, bits)\n",
    "    time_taken_find_all = time.perf_counter() - t0\n",
    "\n",
    "    # Set-up find_all_single_permutation:\n",
    "    delayed_funcs = [joblib.delayed(simhash.find_all_single_permutation)(hashes, i, blocks, bits) for i in range(nck)]\n",
    "    pool = joblib.Parallel(\n",
    "        n_jobs=num_cores,\n",
    "        backend=\"loky\",\n",
    "    )\n",
    "\n",
    "    # Time run-time of find_all_single_permutation:\n",
    "    t0 = time.perf_counter()\n",
    "    matches_perm = pool(delayed_funcs)\n",
    "    time_taken_perm = time.perf_counter() - t0\n",
    "    \n",
    "    if not skip_checks:\n",
    "        matches_perm = [i for s in matches_perm for i in s]\n",
    "        assert set(matches) == set(matches_perm)\n",
    "\n",
    "    print(f\"num hashes: {n:,.0f}, find_all {time_taken_find_all:.2f}s, perm: {time_taken_perm:.2f}s\")\n",
    "    run_results[\"time_taken_find_all\"].append(time_taken_find_all)\n",
    "    run_results[\"time_taken_perm\"].append(time_taken_perm)\n",
    "    run_results[\"num_hashes\"].append(n)\n",
    "    run_results[\"fraction_unique\"].append(f)\n",
    "results_df = pd.DataFrame(run_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "num_fracs = len(frac_unique)\n",
    "f, ax = plt.subplots(num_fracs, 1, figsize=(16, int(num_fracs*4)), sharex=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, frac in enumerate(frac_unique):\n",
    "    plot_df = results_df.loc[results_df[\"fraction_unique\"] == frac]\n",
    "    pct_diff = 1 - plot_df[\"time_taken_perm\"] / plot_df[\"time_taken_find_all\"]\n",
    "    ax[i].plot(plot_df[\"num_hashes\"].values, pct_diff *100, marker=\".\", markersize=14, color=\"skyblue\");\n",
    "    ax[i].axhline(0, color=\"red\", linestyle=\"--\");\n",
    "    ax[i].set_ylabel(\"% runtime saving\");\n",
    "    ax[i].set_title(f\"Fraction of unique hashes: {frac}\");\n",
    "ax[i].set_xlabel(\"Number of hashes\");\n",
    "\n",
    "x_values = ax[i].get_xticks();\n",
    "ax[i].set_xticklabels([\"{:.0e}\".format(x) for x in x_values]);\n",
    "plt.suptitle(f\"Plot showing runtime % saving vs. find_all \\n Blocks: {blocks}, bits: {bits}, num cores: {num_cores}\", y=0.95);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simhash",
   "language": "python",
   "name": "simhash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
